{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f2bedeb",
   "metadata": {},
   "source": [
    "# 🔄 Pipeline ETL - Dados ITBI Recife 2023-2025\n",
    "\n",
    "**Projeto de Integração de Dados**  \n",
    "**ETL = Extract → Transform → Load**\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Objetivo\n",
    "Implementar um pipeline ETL completo para integrar dados do ITBI (Imposto sobre Transmissão de Bens Imóveis) da cidade do Recife dos anos 2023, 2024 e 2025.\n",
    "\n",
    "## 🔄 Diferença ETL vs ELT:\n",
    "- **ETL**: Extract → Transform → Load (transforma durante a extração)\n",
    "- **ELT**: Extract → Load → Transform (carrega dados brutos, transforma no banco)\n",
    "\n",
    "## 📊 Fontes de Dados:\n",
    "- Portal de Dados Abertos do Recife\n",
    "- Datasets ITBI 2023, 2024, 2025\n",
    "- Formato: CSV com separador ';'\n",
    "\n",
    "## 🎯 Vantagens do ETL:\n",
    "- Controle total sobre transformações\n",
    "- Dados chegam limpos no destino\n",
    "- Menor uso de recursos do banco\n",
    "- Implementação mais simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcdc38a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dependências instaladas e configuradas!\n",
      "📅 Execução ETL iniciada em: 2025-07-29 17:16:50\n",
      "🔄 Pipeline: Extract → Transform → Load\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurações de visualização\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"✅ Dependências instaladas e configuradas!\")\n",
    "print(f\"📅 Execução ETL iniciada em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"🔄 Pipeline: Extract → Transform → Load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58cee8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 INICIANDO EXTRAÇÃO DOS DADOS - PIPELINE ETL\n",
      "---------------------------------------------\n",
      "🔍 No ETL, extraímos e validamos os dados\n",
      "\n",
      "📅 Extraindo dados ITBI 2023...\n",
      "   ✅ Dados extraídos: 12,669 registros\n",
      "   📊 Colunas encontradas: 25\n",
      "   📝 Metadados adicionados\n",
      "\n",
      "📅 Extraindo dados ITBI 2024...\n",
      "   ✅ Dados extraídos: 15,242 registros\n",
      "   📊 Colunas encontradas: 25\n",
      "   📝 Metadados adicionados\n",
      "\n",
      "📅 Extraindo dados ITBI 2025...\n",
      "   ✅ Dados extraídos: 7,206 registros\n",
      "   📊 Colunas encontradas: 25\n",
      "   📝 Metadados adicionados\n",
      "\n",
      "✅ Extração ETL concluída:\n",
      "   • Datasets extraídos: 3\n",
      "   • Total de registros: 35,117\n",
      "   • Próximo passo: TRANSFORM (transformações em Python)\n",
      "\n",
      "🔍 ESTRUTURA DOS DADOS EXTRAÍDOS:\n",
      "\n",
      "📅 Dataset 2023:\n",
      "   • Shape: (12669, 25)\n",
      "   • Memória: 13.43 MB\n",
      "   • Valores nulos: 8,124\n",
      "   • Tipos de dados únicos: 4\n",
      "\n",
      "📅 Dataset 2024:\n",
      "   • Shape: (15242, 25)\n",
      "   • Memória: 16.16 MB\n",
      "   • Valores nulos: 12,681\n",
      "   • Tipos de dados únicos: 4\n",
      "\n",
      "📅 Dataset 2025:\n",
      "   • Shape: (7206, 25)\n",
      "   • Memória: 7.64 MB\n",
      "   • Valores nulos: 5,822\n",
      "   • Tipos de dados únicos: 4\n"
     ]
    }
   ],
   "source": [
    "def extract_itbi_data_etl():\n",
    "    \"\"\"\n",
    "    Extrai dados ITBI dos 3 anos para o pipeline ETL.\n",
    "    No ETL, extraímos os dados e aplicamos validações básicas.\n",
    "    \"\"\"\n",
    "    \n",
    "    # URLs dos datasets oficiais\n",
    "    datasets_urls = [\n",
    "        (\"2023\", \"http://dados.recife.pe.gov.br/dataset/28e3e25e-a9a7-4a9f-90a8-bb02d09cbc18/resource/d0c08a6f-4c27-423c-9219-8d13403816f4/download/itbi_2023.csv\"),\n",
    "        (\"2024\", \"http://dados.recife.pe.gov.br/dataset/28e3e25e-a9a7-4a9f-90a8-bb02d09cbc18/resource/a36d548b-d705-496a-ac47-4ec36f068474/download/itbi_2024.csv\"),\n",
    "        (\"2025\", \"http://dados.recife.pe.gov.br/dataset/28e3e25e-a9a7-4a9f-90a8-bb02d09cbc18/resource/5b582147-3935-459a-bbf7-ee623c22c97b/download/itbi_2025.csv\")\n",
    "    ]\n",
    "    \n",
    "    datasets_dict = {}\n",
    "    total_records = 0\n",
    "    \n",
    "    print(\"📥 INICIANDO EXTRAÇÃO DOS DADOS - PIPELINE ETL\")\n",
    "    print(\"-\" * 45)\n",
    "    print(\"🔍 No ETL, extraímos e validamos os dados\")\n",
    "    \n",
    "    for year, url in datasets_urls:\n",
    "        print(f\"\\n📅 Extraindo dados ITBI {year}...\")\n",
    "        \n",
    "        try:\n",
    "            # Carregar dados com validações\n",
    "            df = pd.read_csv(url, sep=';', encoding='utf-8')\n",
    "            \n",
    "            # Adicionar metadados\n",
    "            df['source_year'] = year\n",
    "            df['extraction_timestamp'] = datetime.now()\n",
    "            df['pipeline_type'] = 'ETL'\n",
    "            \n",
    "            # Validações básicas durante extração\n",
    "            if df.empty:\n",
    "                raise ValueError(f\"Dataset {year} está vazio\")\n",
    "            \n",
    "            if len(df.columns) < 10:\n",
    "                raise ValueError(f\"Dataset {year} tem poucas colunas: {len(df.columns)}\")\n",
    "            \n",
    "            # Verificar colunas essenciais\n",
    "            required_columns = ['valor_avaliacao', 'bairro', 'data_transacao']\n",
    "            missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "            if missing_columns:\n",
    "                print(f\"   ⚠️ Colunas essenciais faltando: {missing_columns}\")\n",
    "            \n",
    "            # Armazenar dataset\n",
    "            datasets_dict[year] = df\n",
    "            total_records += len(df)\n",
    "            \n",
    "            print(f\"   ✅ Dados extraídos: {len(df):,} registros\")\n",
    "            print(f\"   📊 Colunas encontradas: {len(df.columns)}\")\n",
    "            print(f\"   📝 Metadados adicionados\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Erro extraindo {year}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n✅ Extração ETL concluída:\")\n",
    "    print(f\"   • Datasets extraídos: {len(datasets_dict)}\")\n",
    "    print(f\"   • Total de registros: {total_records:,}\")\n",
    "    print(f\"   • Próximo passo: TRANSFORM (transformações em Python)\")\n",
    "    \n",
    "    return datasets_dict\n",
    "\n",
    "# Executar extração ETL\n",
    "datasets_raw_etl = extract_itbi_data_etl()\n",
    "\n",
    "# Mostrar estrutura dos dados extraídos\n",
    "print(\"\\n🔍 ESTRUTURA DOS DADOS EXTRAÍDOS:\")\n",
    "for year, df in datasets_raw_etl.items():\n",
    "    print(f\"\\n📅 Dataset {year}:\")\n",
    "    print(f\"   • Shape: {df.shape}\")\n",
    "    print(f\"   • Memória: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"   • Valores nulos: {df.isnull().sum().sum():,}\")\n",
    "    print(f\"   • Tipos de dados únicos: {len(df.dtypes.unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9da018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Funções de transformação ETL definidas!\n"
     ]
    }
   ],
   "source": [
    "def fix_encoding(text):\n",
    "    \"\"\"Corrige problemas de encoding em texto.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    try:\n",
    "        return text.encode('latin1').decode('utf-8')\n",
    "    except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "        return text\n",
    "\n",
    "def convert_currency_format(value):\n",
    "    \"\"\"Converte formato monetário brasileiro para internacional.\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return value\n",
    "    return str(value).replace(',', '.')\n",
    "\n",
    "def clean_column_names(df):\n",
    "    \"\"\"Limpa e padroniza nomes de colunas.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"   🧹 Limpando nomes das colunas...\")\n",
    "    \n",
    "    # Manter nomes originais mas criar mapeamento se necessário\n",
    "    original_columns = len(df.columns)\n",
    "    \n",
    "    # Remover colunas completamente vazias\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    \n",
    "    removed_columns = original_columns - len(df.columns)\n",
    "    if removed_columns > 0:\n",
    "        print(f\"   • Colunas vazias removidas: {removed_columns}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"Trata valores nulos no dataset.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"   🔧 Tratando valores nulos...\")\n",
    "    \n",
    "    # Contar nulos antes\n",
    "    nulls_before = df.isnull().sum().sum()\n",
    "    \n",
    "    # Tratar campos de texto\n",
    "    text_columns = df.select_dtypes(include=['object']).columns\n",
    "    for col in text_columns:\n",
    "        if col not in ['source_year', 'extraction_timestamp', 'pipeline_type']:\n",
    "            df[col] = df[col].fillna('Não informado')\n",
    "    \n",
    "    # Contar nulos depois\n",
    "    nulls_after = df.isnull().sum().sum()\n",
    "    \n",
    "    print(f\"   • Nulos tratados: {nulls_before:,} → {nulls_after:,}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fix_data_types(df):\n",
    "    \"\"\"Corrige tipos de dados das colunas.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"   🔢 Corrigindo tipos de dados...\")\n",
    "    \n",
    "    # Identificar colunas numéricas por padrão nos nomes\n",
    "    numeric_patterns = ['valor', 'area', 'ano', 'sfh', 'fracao']\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        \n",
    "        # Se é uma coluna que deve ser numérica\n",
    "        if any(pattern in col_lower for pattern in numeric_patterns):\n",
    "            if col not in ['source_year', 'extraction_timestamp', 'pipeline_type']:\n",
    "                try:\n",
    "                    # Converter vírgulas para pontos\n",
    "                    df[col] = df[col].astype(str).str.replace(',', '.')\n",
    "                    # Remover pontos de milhares\n",
    "                    df[col] = df[col].str.replace(r'(\\d)\\.(\\d{3})', r'\\1\\2', regex=True)\n",
    "                    # Converter para float\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                    print(f\"   • {col}: convertido para numérico\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   ⚠️ Erro convertendo {col}: {e}\")\n",
    "    \n",
    "    # Converter datas\n",
    "    date_columns = [col for col in df.columns if 'data' in col.lower()]\n",
    "    for col in date_columns:\n",
    "        try:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            print(f\"   • {col}: convertido para datetime\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Erro convertendo data {col}: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_derived_metrics(df):\n",
    "    \"\"\"Cria métricas derivadas durante a transformação.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"   🧮 Criando métricas derivadas...\")\n",
    "    \n",
    "    # Valor por m²\n",
    "    if 'valor_avaliacao' in df.columns and 'area_construida' in df.columns:\n",
    "        df['valor_por_m2'] = (df['valor_avaliacao'] / df['area_construida']).round(2)\n",
    "        print(\"   • valor_por_m2: criado\")\n",
    "    \n",
    "    # Idade do imóvel\n",
    "    if 'data_transacao' in df.columns and 'ano_construcao' in df.columns:\n",
    "        try:\n",
    "            df['ano_transacao'] = df['data_transacao'].dt.year\n",
    "            df['idade_imovel'] = df['ano_transacao'] - df['ano_construcao']\n",
    "            print(\"   • idade_imovel: criado\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Erro criando idade_imovel: {e}\")\n",
    "    \n",
    "    # Faixa de valor\n",
    "    if 'valor_avaliacao' in df.columns:\n",
    "        def classificar_valor(valor):\n",
    "            if pd.isna(valor):\n",
    "                return 'Não informado'\n",
    "            if valor <= 200000:\n",
    "                return 'Baixo (até R$ 200k)'\n",
    "            elif valor <= 500000:\n",
    "                return 'Médio (R$ 200k-500k)'\n",
    "            elif valor <= 1000000:\n",
    "                return 'Alto (R$ 500k-1M)'\n",
    "            else:\n",
    "                return 'Premium (acima R$ 1M)'\n",
    "        \n",
    "        df['faixa_valor'] = df['valor_avaliacao'].apply(classificar_valor)\n",
    "        print(\"   • faixa_valor: criado\")\n",
    "    \n",
    "    # Indicador de financiamento\n",
    "    if 'sfh' in df.columns:\n",
    "        df['tem_financiamento'] = (df['sfh'] > 0).astype(int)\n",
    "        print(\"   • tem_financiamento: criado\")\n",
    "    \n",
    "    # Componentes temporais\n",
    "    if 'data_transacao' in df.columns:\n",
    "        try:\n",
    "            df['mes_transacao'] = df['data_transacao'].dt.month\n",
    "            df['trimestre'] = df['data_transacao'].dt.quarter\n",
    "            print(\"   • componentes temporais: criados\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Erro criando componentes temporais: {e}\")\n",
    "    \n",
    "    # Categoria de área\n",
    "    if 'area_construida' in df.columns:\n",
    "        def classificar_area(area):\n",
    "            if pd.isna(area):\n",
    "                return 'Não informado'\n",
    "            if area <= 50:\n",
    "                return 'Pequeno (até 50m²)'\n",
    "            elif area <= 100:\n",
    "                return 'Médio (50-100m²)'\n",
    "            elif area <= 200:\n",
    "                return 'Grande (100-200m²)'\n",
    "            else:\n",
    "                return 'Extra Grande (acima 200m²)'\n",
    "        \n",
    "        df['categoria_area'] = df['area_construida'].apply(classificar_area)\n",
    "        print(\"   • categoria_area: criado\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"✅ Funções de transformação ETL definidas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fb7ac8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 INICIANDO TRANSFORMAÇÕES ETL\n",
      "========================================\n",
      "🎯 No ETL, transformamos os dados EM MEMÓRIA antes de carregar\n",
      "\n",
      "🔄 Transformando dataset 2023 (ETL):\n",
      "-----------------------------------\n",
      "   🧹 Limpando nomes das colunas...\n",
      "   🔧 Tratando valores nulos...\n",
      "   • Nulos tratados: 8,124 → 6,804\n",
      "   🔢 Corrigindo tipos de dados...\n",
      "   • valor_avaliacao: convertido para numérico\n",
      "   • ano_construcao: convertido para numérico\n",
      "   • area_terreno: convertido para numérico\n",
      "   • area_construida: convertido para numérico\n",
      "   • fracao_ideal: convertido para numérico\n",
      "   • sfh: convertido para numérico\n",
      "   • ano: convertido para numérico\n",
      "   • data_transacao: convertido para datetime\n",
      "   🧮 Criando métricas derivadas...\n",
      "   • valor_por_m2: criado\n",
      "   • idade_imovel: criado\n",
      "   • faixa_valor: criado\n",
      "   • tem_financiamento: criado\n",
      "   • componentes temporais: criados\n",
      "   • categoria_area: criado\n",
      "   ✅ Transformação 2023 concluída:\n",
      "      • Shape: (12669, 25) → (12669, 33)\n",
      "      • Colunas adicionadas: 8\n",
      "\n",
      "🔄 Transformando dataset 2024 (ETL):\n",
      "-----------------------------------\n",
      "   🧹 Limpando nomes das colunas...\n",
      "   🔧 Tratando valores nulos...\n",
      "   • Nulos tratados: 12,681 → 11,238\n",
      "   🔢 Corrigindo tipos de dados...\n",
      "   • valor_avaliacao: convertido para numérico\n",
      "   • ano_construcao: convertido para numérico\n",
      "   • area_terreno: convertido para numérico\n",
      "   • area_construida: convertido para numérico\n",
      "   • fracao_ideal: convertido para numérico\n",
      "   • sfh: convertido para numérico\n",
      "   • ano: convertido para numérico\n",
      "   • data_transacao: convertido para datetime\n",
      "   🧮 Criando métricas derivadas...\n",
      "   • valor_por_m2: criado\n",
      "   • idade_imovel: criado\n",
      "   • faixa_valor: criado\n",
      "   • tem_financiamento: criado\n",
      "   • componentes temporais: criados\n",
      "   • categoria_area: criado\n",
      "   ✅ Transformação 2024 concluída:\n",
      "      • Shape: (15242, 25) → (15242, 33)\n",
      "      • Colunas adicionadas: 8\n",
      "\n",
      "🔄 Transformando dataset 2025 (ETL):\n",
      "-----------------------------------\n",
      "   🧹 Limpando nomes das colunas...\n",
      "   🔧 Tratando valores nulos...\n",
      "   • Nulos tratados: 5,822 → 5,246\n",
      "   🔢 Corrigindo tipos de dados...\n",
      "   • valor_avaliacao: convertido para numérico\n",
      "   • ano_construcao: convertido para numérico\n",
      "   • area_terreno: convertido para numérico\n",
      "   • area_construida: convertido para numérico\n",
      "   • fracao_ideal: convertido para numérico\n",
      "   • sfh: convertido para numérico\n",
      "   • ano: convertido para numérico\n",
      "   • data_transacao: convertido para datetime\n",
      "   🧮 Criando métricas derivadas...\n",
      "   • valor_por_m2: criado\n",
      "   • idade_imovel: criado\n",
      "   • faixa_valor: criado\n",
      "   • tem_financiamento: criado\n",
      "   • componentes temporais: criados\n",
      "   • categoria_area: criado\n",
      "   ✅ Transformação 2025 concluída:\n",
      "      • Shape: (7206, 25) → (7206, 33)\n",
      "      • Colunas adicionadas: 8\n",
      "\n",
      "✅ Todas as transformações ETL concluídas!\n",
      "   ⏱️ Tempo de processamento: 0:00:00.443133\n",
      "   💾 Próximo passo: LOAD (carregar dados transformados)\n",
      "\n",
      "📊 QUALIDADE DAS TRANSFORMAÇÕES:\n",
      "-----------------------------------\n",
      "\n",
      "📅 Dataset 2023:\n",
      "   • Registros: 12,669\n",
      "   • Colunas: 33\n",
      "   • Memória: 12.41 MB\n",
      "   • Nulos restantes: 6,804\n",
      "   • Métricas derivadas: 4/4\n",
      "\n",
      "📅 Dataset 2024:\n",
      "   • Registros: 15,242\n",
      "   • Colunas: 33\n",
      "   • Memória: 14.91 MB\n",
      "   • Nulos restantes: 11,238\n",
      "   • Métricas derivadas: 4/4\n",
      "\n",
      "📅 Dataset 2025:\n",
      "   • Registros: 7,206\n",
      "   • Colunas: 33\n",
      "   • Memória: 7.05 MB\n",
      "   • Nulos restantes: 5,246\n",
      "   • Métricas derivadas: 4/4\n",
      "\n",
      "🎯 RESUMO GERAL:\n",
      "   • Total de registros transformados: 35,117\n",
      "   • Anos processados: 3\n",
      "   • Pipeline: Dados transformados EM MEMÓRIA\n"
     ]
    }
   ],
   "source": [
    "def transform_dataset_etl(df, year):\n",
    "    \"\"\"Aplica todas as transformações ETL em um dataset.\"\"\"\n",
    "    \n",
    "    print(f\"\\n🔄 Transformando dataset {year} (ETL):\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    original_shape = df.shape\n",
    "    \n",
    "    # Pipeline de transformações ETL\n",
    "    df = clean_column_names(df)\n",
    "    df = handle_missing_values(df)\n",
    "    df = fix_data_types(df)\n",
    "    df = create_derived_metrics(df)\n",
    "    \n",
    "    final_shape = df.shape\n",
    "    \n",
    "    print(f\"   ✅ Transformação {year} concluída:\")\n",
    "    print(f\"      • Shape: {original_shape} → {final_shape}\")\n",
    "    print(f\"      • Colunas adicionadas: {final_shape[1] - original_shape[1]}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def transform_all_datasets_etl(datasets_dict):\n",
    "    \"\"\"Transforma todos os datasets usando ETL.\"\"\"\n",
    "    \n",
    "    print(\"\\n🔄 INICIANDO TRANSFORMAÇÕES ETL\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"🎯 No ETL, transformamos os dados EM MEMÓRIA antes de carregar\")\n",
    "    \n",
    "    transformed_datasets = {}\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    for year, df in datasets_dict.items():\n",
    "        transformed_df = transform_dataset_etl(df, year)\n",
    "        transformed_datasets[year] = transformed_df\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\n✅ Todas as transformações ETL concluídas!\")\n",
    "    print(f\"   ⏱️ Tempo de processamento: {processing_time}\")\n",
    "    print(f\"   💾 Próximo passo: LOAD (carregar dados transformados)\")\n",
    "    \n",
    "    return transformed_datasets\n",
    "\n",
    "# Executar transformações ETL\n",
    "datasets_transformed_etl = transform_all_datasets_etl(datasets_raw_etl)\n",
    "\n",
    "# Verificar qualidade das transformações\n",
    "print(\"\\n📊 QUALIDADE DAS TRANSFORMAÇÕES:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "total_records = 0\n",
    "for year, df in datasets_transformed_etl.items():\n",
    "    print(f\"\\n📅 Dataset {year}:\")\n",
    "    print(f\"   • Registros: {len(df):,}\")\n",
    "    print(f\"   • Colunas: {len(df.columns)}\")\n",
    "    print(f\"   • Memória: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"   • Nulos restantes: {df.isnull().sum().sum():,}\")\n",
    "    \n",
    "    # Verificar métricas derivadas\n",
    "    derived_metrics = ['valor_por_m2', 'faixa_valor', 'tem_financiamento', 'idade_imovel']\n",
    "    available_metrics = [col for col in derived_metrics if col in df.columns]\n",
    "    print(f\"   • Métricas derivadas: {len(available_metrics)}/{len(derived_metrics)}\")\n",
    "    \n",
    "    total_records += len(df)\n",
    "\n",
    "print(f\"\\n🎯 RESUMO GERAL:\")\n",
    "print(f\"   • Total de registros transformados: {total_records:,}\")\n",
    "print(f\"   • Anos processados: {len(datasets_transformed_etl)}\")\n",
    "print(f\"   • Pipeline: Dados transformados EM MEMÓRIA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
