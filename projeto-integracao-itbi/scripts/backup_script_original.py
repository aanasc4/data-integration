# SCRIPT SIMPLIFICADO - ETL + AN√ÅLISES (SEM GR√ÅFICOS)
# Projeto de Integra√ß√£o ITBI Recife 2023-2025
# UFPE - Centro de Inform√°tica - Banco de Dados 2025.1

import numpy as np
import pandas as pd
import sqlite3
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

print("üè† PROJETO INTEGRA√á√ÉO ITBI RECIFE - VERS√ÉO SIMPLIFICADA")
print("=" * 60)
print("UFPE - Centro de Inform√°tica - Banco de Dados 2025.1")
print("=" * 60)

start_time = datetime.now()
print(f"In√≠cio da execu√ß√£o: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")

# ============================================================================
# FASE 1: EXTRA√á√ÉO DOS DADOS (ETL - EXTRACT)
# ============================================================================

print("\nüì• FASE 1: EXTRA√á√ÉO DOS DADOS")
print("-" * 40)

# Configura√ß√£o dos datasets
dataset_directory = "datasets"
os.makedirs(dataset_directory, exist_ok=True)

datasets_urls = [
    ("2023", "http://dados.recife.pe.gov.br/dataset/28e3e25e-a9a7-4a9f-90a8-bb02d09cbc18/resource/d0c08a6f-4c27-423c-9219-8d13403816f4/download/itbi_2023.csv"),
    ("2024", "http://dados.recife.pe.gov.br/dataset/28e3e25e-a9a7-4a9f-90a8-bb02d09cbc18/resource/a36d548b-d705-496a-ac47-4ec36f068474/download/itbi_2024.csv"),
    ("2025", "http://dados.recife.pe.gov.br/dataset/28e3e25e-a9a7-4a9f-90a8-bb02d09cbc18/resource/5b582147-3935-459a-bbf7-ee623c22c97b/download/itbi_2025.csv")
]

# Extrair dados
datasets_dict = {}
total_records = 0

for year, url in datasets_urls:
    print(f"\nüìÖ Carregando ITBI {year}...")
    try:
        df = pd.read_csv(url, sep=';', encoding='utf-8')
        df['year'] = int(year)
        datasets_dict[year] = df
        
        print(f"   ‚úÖ {len(df):,} registros carregados")
        print(f"   üìä Colunas: {len(df.columns)}")
        total_records += len(df)
        
    except Exception as e:
        print(f"   ‚ùå Erro: {e}")

print(f"\n‚úÖ Extra√ß√£o conclu√≠da: {total_records:,} registros totais")

# ============================================================================
# FASE 2: TRANSFORMA√á√ÉO DOS DADOS (ETL - TRANSFORM)
# ============================================================================

print("\nüîÑ FASE 2: TRANSFORMA√á√ÉO DOS DADOS")
print("-" * 40)

# Fun√ß√£o para corrigir encoding
def fix_encoding(text):
    if not isinstance(text, str):
        return text
    try:
        return text.encode('latin1').decode('utf-8')
    except:
        return text

# Fun√ß√£o para converter formato brasileiro para internacional
def convert_currency(value):
    if pd.isna(value):
        return value
    return str(value).replace(',', '.')

print("üßπ Aplicando transforma√ß√µes...")

for year, df in datasets_dict.items():
    print(f"\nüìÖ Transformando dataset {year}:")
    
    # 1. Remover colunas redundantes
    columns_to_drop = []
    if 'cidade' in df.columns:
        columns_to_drop.extend(['cidade', 'uf'])
    if columns_to_drop:
        df = df.drop(columns_to_drop, axis=1)
        print(f"   ‚Ä¢ Colunas removidas: {columns_to_drop}")
    
    # 2. Renomear coluna SFH
    if 'sfh' in df.columns:
        df = df.rename(columns={'sfh': 'valores_financiados_sfh'})
        print("   ‚Ä¢ Coluna SFH renomeada")
    
    # 3. Tratar valores nulos
    null_before = df.isnull().sum().sum()
    if 'complemento' in df.columns:
        df['complemento'] = df['complemento'].fillna('Sem complemento')
    null_after = df.isnull().sum().sum()
    print(f"   ‚Ä¢ Nulos antes: {null_before}, depois: {null_after}")
    
    # 4. Corrigir encoding em colunas de texto
    text_columns = df.select_dtypes(include=['object']).columns
    for col in text_columns:
        if col not in ['valores_financiados_sfh', 'valor_avaliacao', 'area_terreno', 'area_construida']:
            try:
                df[col] = df[col].apply(fix_encoding)
            except:
                pass
    print("   ‚Ä¢ Encoding corrigido")
    
    # 5. Converter tipos de dados
    monetary_columns = ['valor_avaliacao', 'area_terreno', 'area_construida']
    if 'valores_financiados_sfh' in df.columns:
        monetary_columns.append('valores_financiados_sfh')
    
    for col in monetary_columns:
        if col in df.columns:
            try:
                df[col] = df[col].apply(convert_currency).astype(float)
                print(f"   ‚Ä¢ {col} convertido para float")
            except Exception as e:
                print(f"   ‚ö†Ô∏è Erro convertendo {col}: {e}")
    
    # 6. Converter datas
    if 'data_transacao' in df.columns:
        try:
            df['data_transacao'] = pd.to_datetime(df['data_transacao'])
            print("   ‚Ä¢ Datas convertidas")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erro convertendo datas: {e}")
    
    # Atualizar dataset
    datasets_dict[year] = df

print("\n‚úÖ Transforma√ß√µes conclu√≠das")

# ============================================================================
# FASE 3: CONSOLIDA√á√ÉO (ETL - LOAD)
# ============================================================================

print("\nüíæ FASE 3: CONSOLIDA√á√ÉO DOS DADOS")
print("-" * 40)

# Consolidar todos os datasets
df_consolidated = pd.concat(list(datasets_dict.values()), ignore_index=True)

print(f"‚úÖ Consolida√ß√£o conclu√≠da:")
print(f"   ‚Ä¢ Total de registros: {len(df_consolidated):,}")
print(f"   ‚Ä¢ Total de colunas: {len(df_consolidated.columns)}")
print(f"   ‚Ä¢ Per√≠odo: {df_consolidated['year'].min()}-{df_consolidated['year'].max()}")

# Verificar colunas dispon√≠veis
print(f"\nüìã Colunas dispon√≠veis:")
for i, col in enumerate(df_consolidated.columns):
    print(f"   {i+1:2d}. {col}")

# Criar m√©tricas derivadas
print("\nüßÆ Criando m√©tricas derivadas...")

# Valor por m¬≤ (se poss√≠vel)
try:
    df_consolidated['valor_por_m2'] = (
        df_consolidated['valor_avaliacao'] / df_consolidated['area_construida']
    ).round(2)
    print("   ‚úÖ valor_por_m2 criado")
except Exception as e:
    print(f"   ‚ö†Ô∏è Erro criando valor_por_m2: {e}")

# Idade do im√≥vel (se poss√≠vel)
try:
    df_consolidated['idade_imovel'] = (
        df_consolidated['data_transacao'].dt.year - df_consolidated['ano_construcao']
    )
    print("   ‚úÖ idade_imovel criado")
except Exception as e:
    print(f"   ‚ö†Ô∏è Erro criando idade_imovel: {e}")

# Faixa de valor
try:
    def classificar_valor(valor):
        if pd.isna(valor):
            return 'N√£o informado'
        if valor <= 200000:
            return 'Baixo (at√© R$ 200k)'
        elif valor <= 500000:
            return 'M√©dio (R$ 200k-500k)'
        elif valor <= 1000000:
            return 'Alto (R$ 500k-1M)'
        else:
            return 'Premium (acima R$ 1M)'

    df_consolidated['faixa_valor'] = df_consolidated['valor_avaliacao'].apply(classificar_valor)
    print("   ‚úÖ faixa_valor criado")
except Exception as e:
    print(f"   ‚ö†Ô∏è Erro criando faixa_valor: {e}")

# Indicador de financiamento
try:
    if 'valores_financiados_sfh' in df_consolidated.columns:
        df_consolidated['tem_financiamento'] = (df_consolidated['valores_financiados_sfh'] > 0)
        print("   ‚úÖ tem_financiamento criado")
except Exception as e:
    print(f"   ‚ö†Ô∏è Erro criando tem_financiamento: {e}")

# Componentes temporais
try:
    df_consolidated['mes_transacao'] = df_consolidated['data_transacao'].dt.month
    df_consolidated['trimestre'] = df_consolidated['data_transacao'].dt.quarter
    print("   ‚úÖ componentes temporais criados")
except Exception as e:
    print(f"   ‚ö†Ô∏è Erro criando componentes temporais: {e}")

# Salvar dataset consolidado
consolidated_path = f"{dataset_directory}/itbi_consolidado_2023_2025.csv"
df_consolidated.to_csv(consolidated_path, sep=';', encoding='utf-8', index=False)
print(f"üíæ Dataset salvo: {consolidated_path}")

# ============================================================================
# AN√ÅLISE 1: EVOLU√á√ÉO TEMPORAL DOS PRE√áOS
# ============================================================================

print("\nüìä AN√ÅLISE 1: EVOLU√á√ÉO TEMPORAL DOS PRE√áOS")
print("-" * 50)

try:
    # Calcular evolu√ß√£o por ano
    evolucao_anual = df_consolidated.groupby('year').agg({
        'valor_avaliacao': ['count', 'mean', 'median']
    }).round(2)

    # Achatar colunas
    evolucao_anual.columns = ['transacoes', 'valor_medio', 'valor_mediano']

    print("üìà Evolu√ß√£o Anual:")
    print(evolucao_anual)

    # Calcular varia√ß√µes
    anos = sorted(df_consolidated['year'].unique())
    if len(anos) >= 2:
        var_2023_2024 = ((evolucao_anual.loc[2024, 'valor_medio'] - evolucao_anual.loc[2023, 'valor_medio']) / evolucao_anual.loc[2023, 'valor_medio']) * 100
        
        if 2025 in evolucao_anual.index:
            var_2024_2025 = ((evolucao_anual.loc[2025, 'valor_medio'] - evolucao_anual.loc[2024, 'valor_medio']) / evolucao_anual.loc[2024, 'valor_medio']) * 100
        else:
            var_2024_2025 = 0

        print(f"\nüí° INSIGHTS AN√ÅLISE 1:")
        print(f"   ‚Ä¢ Varia√ß√£o 2023‚Üí2024: {var_2023_2024:+.1f}%")
        if var_2024_2025 != 0:
            print(f"   ‚Ä¢ Varia√ß√£o 2024‚Üí2025: {var_2024_2025:+.1f}%")
        print(f"   ‚Ä¢ Ano com mais transa√ß√µes: {evolucao_anual['transacoes'].idxmax()}")
        print(f"   ‚Ä¢ Maior valor m√©dio: R$ {evolucao_anual['valor_medio'].max():,.2f}")

except Exception as e:
    print(f"‚ùå Erro na An√°lise 1: {e}")

# ============================================================================
# AN√ÅLISE 2: AN√ÅLISE GEOGR√ÅFICA POR BAIRROS
# ============================================================================

print("\nüó∫Ô∏è AN√ÅLISE 2: AN√ÅLISE GEOGR√ÅFICA POR BAIRROS")
print("-" * 50)

try:
    # Top bairros por transa√ß√µes
    if 'bairro' in df_consolidated.columns:
        top_bairros = df_consolidated.groupby('bairro').agg({
            'valor_avaliacao': ['count', 'mean']
        }).round(2)

        top_bairros.columns = ['transacoes', 'valor_medio']
        top_bairros = top_bairros.sort_values('transacoes', ascending=False)

        print("üèÜ TOP 10 BAIRROS POR TRANSA√á√ïES:")
        print(top_bairros.head(10))

        # Top por valor
        top_valor = top_bairros.sort_values('valor_medio', ascending=False)
        print("\nüí∞ TOP 10 BAIRROS POR VALOR:")
        print(top_valor.head(10))

        bairro_mais_ativo = top_bairros.index[0]
        bairro_mais_caro = top_valor.index[0]

        print(f"\nüí° INSIGHTS AN√ÅLISE 2:")
        print(f"   ‚Ä¢ Bairro mais ativo: {bairro_mais_ativo}")
        print(f"   ‚Ä¢ Bairro mais valorizado: {bairro_mais_caro}")
        print(f"   ‚Ä¢ Total de bairros: {df_consolidated['bairro'].nunique()}")
        print(f"   ‚Ä¢ Concentra√ß√£o: Top 10 = {top_bairros.head(10)['transacoes'].sum():,} transa√ß√µes")

except Exception as e:
    print(f"‚ùå Erro na An√°lise 2: {e}")

# ============================================================================
# AN√ÅLISE 3: PERFIL DE IM√ìVEIS E FINANCIAMENTO
# ============================================================================

print("\nüè¶ AN√ÅLISE 3: PERFIL DE IM√ìVEIS E FINANCIAMENTO")
print("-" * 55)

try:
    # An√°lise por tipo de im√≥vel
    if 'tipo_imovel' in df_consolidated.columns:
        tipos_imoveis = df_consolidated.groupby('tipo_imovel').agg({
            'valor_avaliacao': ['count', 'mean']
        }).round(2)

        tipos_imoveis.columns = ['quantidade', 'valor_medio']
        tipos_imoveis = tipos_imoveis.sort_values('quantidade', ascending=False)

        print("üèòÔ∏è AN√ÅLISE POR TIPO DE IM√ìVEL:")
        print(tipos_imoveis)

        # An√°lise de financiamento
        if 'tem_financiamento' in df_consolidated.columns:
            total_financiados = df_consolidated['tem_financiamento'].sum()
            percentual_financiamento = (total_financiados / len(df_consolidated)) * 100

            print(f"\nüí≥ FINANCIAMENTO GERAL:")
            print(f"   ‚Ä¢ Com financiamento: {total_financiados:,} ({percentual_financiamento:.1f}%)")
            print(f"   ‚Ä¢ Sem financiamento: {len(df_consolidated) - total_financiados:,}")

        # Por faixa de valor
        if 'faixa_valor' in df_consolidated.columns:
            faixas_analise = df_consolidated['faixa_valor'].value_counts()
            print(f"\nüí∞ DISTRIBUI√á√ÉO POR FAIXA:")
            print(faixas_analise)

        tipo_mais_comum = df_consolidated['tipo_imovel'].value_counts().index[0]
        tipo_mais_caro = tipos_imoveis['valor_medio'].idxmax()

        print(f"\nüí° INSIGHTS AN√ÅLISE 3:")
        print(f"   ‚Ä¢ Tipo mais comum: {tipo_mais_comum}")
        print(f"   ‚Ä¢ Tipo mais caro: {tipo_mais_caro}")
        if 'tem_financiamento' in df_consolidated.columns:
            print(f"   ‚Ä¢ Taxa geral financiamento: {percentual_financiamento:.1f}%")
        if 'idade_imovel' in df_consolidated.columns:
            print(f"   ‚Ä¢ Idade m√©dia im√≥veis: {df_consolidated['idade_imovel'].mean():.1f} anos")

except Exception as e:
    print(f"‚ùå Erro na An√°lise 3: {e}")

# ============================================================================
# PIPELINE ELT ALTERNATIVO
# ============================================================================

print("\nüîÑ IMPLEMENTANDO PIPELINE ELT ALTERNATIVO")
print("-" * 50)

try:
    # Criar database SQLite
    db_path = f"{dataset_directory}/itbi_datawarehouse.db"
    conn = sqlite3.connect(db_path)

    print(f"üèõÔ∏è Data Warehouse criado: {db_path}")

    # EXTRACT + LOAD: Carregar dados brutos
    for year, url in datasets_urls:
        try:
            df_raw = pd.read_csv(url, sep=';', encoding='utf-8')
            df_raw['source_year'] = year
            df_raw['load_timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            table_name = f"itbi_raw_{year}"
            df_raw.to_sql(table_name, conn, if_exists='replace', index=False)
            
            print(f"   üì• {table_name}: {len(df_raw):,} registros carregados")
            
        except Exception as e:
            print(f"   ‚ùå Erro carregando {year}: {e}")

    # TRANSFORM: Criar tabela limpa via SQL
    transform_sql = """
    CREATE TABLE IF NOT EXISTS itbi_elt_final AS
    SELECT 
        logradouro,
        numero,
        COALESCE(complemento, 'Sem complemento') as complemento,
        CAST(REPLACE(valor_avaliacao, ',', '.') AS REAL) as valor_avaliacao,
        bairro,
        tipo_imovel,
        data_transacao,
        source_year as year
    FROM (
        SELECT * FROM itbi_raw_2023
        UNION ALL
        SELECT * FROM itbi_raw_2024
        UNION ALL
        SELECT * FROM itbi_raw_2025
    )
    WHERE valor_avaliacao IS NOT NULL
    """

    conn.execute("DROP TABLE IF EXISTS itbi_elt_final")
    conn.execute(transform_sql)
    conn.commit()

    # Verificar resultado ELT
    elt_count = conn.execute("SELECT COUNT(*) FROM itbi_elt_final").fetchone()[0]
    print(f"‚úÖ ELT Transform conclu√≠do: {elt_count:,} registros")

    conn.close()

except Exception as e:
    print(f"‚ùå Erro no ELT: {e}")

# ============================================================================
# RESUMO EXECUTIVO FINAL
# ============================================================================

end_time = datetime.now()
execution_time = end_time - start_time

print(f"\n" + "="*60)
print(f"üìã RESUMO EXECUTIVO FINAL")
print("="*60)

print(f"""
üéØ PROJETO CONCLU√çDO COM SUCESSO!

üìä DADOS INTEGRADOS:
   ‚Ä¢ Registros totais: {len(df_consolidated):,}
   ‚Ä¢ Colunas finais: {len(df_consolidated.columns)}
   ‚Ä¢ Per√≠odo: {df_consolidated['year'].min()}-{df_consolidated['year'].max()}
   ‚Ä¢ Bairros √∫nicos: {df_consolidated['bairro'].nunique() if 'bairro' in df_consolidated.columns else 'N/A'}

üîÑ PIPELINES IMPLEMENTADOS:
   ‚Ä¢ ETL: Extract‚ÜíTransform‚ÜíLoad ‚úÖ
   ‚Ä¢ ELT: Extract‚ÜíLoad‚ÜíTransform ‚úÖ
   ‚Ä¢ An√°lises: 3 an√°lises conclu√≠das ‚úÖ

üìÅ ARQUIVOS GERADOS:
   ‚Ä¢ {consolidated_path}
   ‚Ä¢ {db_path}

‚è±Ô∏è TEMPO DE EXECU√á√ÉO: {execution_time}

üöÄ PR√ìXIMOS PASSOS:
   1. Instalar matplotlib/seaborn para gr√°ficos
   2. Executar vers√£o completa com visualiza√ß√µes
   3. Organizar reposit√≥rio GitHub
   4. Preparar apresenta√ß√£o
""")

print("‚úÖ SCRIPT SIMPLIFICADO EXECUTADO COM SUCESSO!")
print("üéì Dados processados - UFPE CIn 2025.1")

# Salvar log de execu√ß√£o
log_path = f"{dataset_directory}/execution_log.txt"
with open(log_path, 'w', encoding='utf-8') as f:
    f.write(f"Execu√ß√£o realizada em: {start_time}\n")
    f.write(f"Dura√ß√£o: {execution_time}\n")
    f.write(f"Registros processados: {len(df_consolidated):,}\n")
    f.write(f"Status: Sucesso\n")

print(f"üìù Log salvo: {log_path}")
